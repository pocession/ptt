{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/hsieh/Documents/ptt/dict.txt ...\n",
      "Loading model from cache /var/folders/54/w85qh3vd4xv5v6qz8w4t70940000gn/T/jieba.ubaa545e936adb4bd7ef290058a1bf422.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize\n",
      "Run main\n",
      "run sentence check\n",
      "Run word, powered by JieBa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.744 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter one interested word: \n",
      "Run sentence_selector\n",
      "Run bag_of_words\n",
      "Run word_count\n",
      "train models\n",
      "購物 樂桃 香草 潛水 OIST 九州 象鼻岩 自駕  [(0, 0.5708070844632387), (10, 0.45083390741665147), (42, 0.29330600939894064), (169, 0.6204028087464465)]\n"
     ]
    }
   ],
   "source": [
    "# Some practice for word2vec\n",
    "# Get contends from an article and process it into a bag of word (continuous bag-of-word model, CBOW)\n",
    "# Read sentences based on \"，\", not \" \" or \"\\n\"\n",
    "\n",
    "import scipy\n",
    "import numpy\n",
    "import gensim\n",
    "import re\n",
    "import jieba\n",
    "import zhon\n",
    "import csv\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "# Receive a list containing lines of an article\n",
    "# Remove unwanted characters\n",
    "# Retrun a list containing sentences of an article\n",
    "def sentence_check(a):\n",
    "    # The faster way is creating a list, then checking any unwanted characters before replacing it. \n",
    "    # However, this way produces multiple output, which increase some complexity \n",
    "    print(\"run sentence check\")\n",
    "    check_list=a\n",
    "    return_list=[]\n",
    "    pattern = r',|\\.|/|;|\\'|`|\\[|\\]|<|>|\\?|:|\"|\\{|\\}|\\~|!|@|#|\\$|%|\\^|&|\\(|\\)|-|=|\\_|\\+|，|。|、|；|‘|’|【|】|·|！| |…|（|）' \n",
    "    \n",
    "    for element in check_list:\n",
    "        element = re.split(pattern,element)\n",
    "        for t in element:\n",
    "            if t != '':\n",
    "                return_list.append(t)\n",
    "    \n",
    "    # Remove \\n in list\n",
    "    return_list = [x for x in return_list if x != '\\n']\n",
    "    return(return_list)\n",
    "\n",
    "\n",
    "# Read sentences of an article and process them as a bag of words\n",
    "# Use jieba module to recognize chinese words\n",
    "# Need a dictionary of common tradition Chinese words\n",
    "# Need a dictionary of stop words\n",
    "# Return a list of lists of words (still keep the sentence information)\n",
    "def word(a):\n",
    "    print(\"Run word, powered by JieBa\")\n",
    "    input_list = a\n",
    "    jieba.set_dictionary('dict.txt')\n",
    "    stops = open('stop.txt', 'r', encoding='utf8') \n",
    "    stopword = stops.read().split('\\n')\n",
    "    stops.close()\n",
    "    \n",
    "    # Update stop words\n",
    "    stopword.append('\\n')\n",
    "    \n",
    "    new_list=[]\n",
    "    i = 0\n",
    "    \n",
    "    # Remove any stop words from list\n",
    "    while i < len(input_list):\n",
    "        element = input_list[i]\n",
    "        new_list.append([t for t in jieba.cut(element) if t not in stopword])\n",
    "        i+=1\n",
    "    \n",
    "    return(new_list)\n",
    "\n",
    "\n",
    "# Read a list of lists of words\n",
    "# Merge all lists\n",
    "# Return a list of words of an article (a bag of word)\n",
    "def bag_of_words(a):\n",
    "    print(\"Run bag_of_words\")\n",
    "    input_list = a\n",
    "    new_list=[]\n",
    "    i = 0\n",
    "    while i < len(input_list):\n",
    "        new_list += input_list[i]\n",
    "        i += 1\n",
    "    return(new_list)        \n",
    "\n",
    "\n",
    "# Select sentences containing interested words\n",
    "def selector(a,b):\n",
    "    print(\"Run sentence_selector\")\n",
    "    input_list = a\n",
    "    word = b\n",
    "    new_list=[]\n",
    "    if word == \"\":\n",
    "        new_list = input_list\n",
    "        return(new_list)\n",
    "\n",
    "    else:\n",
    "        i = 0\n",
    "        while i < len(input_list):\n",
    "            element = input_list[i]\n",
    "            j = 0\n",
    "            while j < len(element):\n",
    "                t = element[j]\n",
    "                if t == word:\n",
    "                    new_list.append(element)\n",
    "                j+=1\n",
    "            i+=1\n",
    "    \n",
    "    return(new_list)\n",
    "\n",
    "    \n",
    "# Recive a bag of words\n",
    "# Count frequency of a word in a bag of words\n",
    "# Return processed corpus\n",
    "def word_count(a):\n",
    "    print(\"Run word_count\")\n",
    "    input_list = a\n",
    "    from collections import Counter\n",
    "    frequency = Counter(input_list)\n",
    "    c = frequency.items()\n",
    "    return_list=[]\n",
    "    for element in c:\n",
    "        if element[1] > 1:\n",
    "            return_list.append(element[0]) # Remove word only appearing once\n",
    "    return(return_list)\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    print(\"Run main\")\n",
    "    csvfile = open(\"Okinawa_Travel.csv\",newline='')\n",
    "    csvreader = csv.reader(csvfile,delimiter=\",\")\n",
    "    title_list=[]\n",
    "    for row in csvreader:\n",
    "        row = row[0].replace('[遊記]','')\n",
    "        title_list.append(row)\n",
    "    csvfile.close()\n",
    "    sentence_list = sentence_check(title_list)\n",
    "    word_list=word(sentence_list)\n",
    "    selected_words = selector(word_list,input(\"Enter one interested word: \"))\n",
    "    bag_word = bag_of_words(word_list) # Pool all words in one list\n",
    "    processed_word_of_bag = word_count(bag_word) # Count frequency of each word, return words appearing more than once\n",
    "    processed_corpus = []\n",
    "    for element in word_list:\n",
    "        temp_list=[]\n",
    "        i = 0\n",
    "        while i < len(element):\n",
    "            for a in processed_word_of_bag:\n",
    "                if element[i] == a:\n",
    "                    temp_list.append(element[i])\n",
    "            i+=1\n",
    "            \n",
    "        b = temp_list\n",
    "        if b != []:\n",
    "            processed_corpus.append(b)\n",
    "    \n",
    "    dictionary = corpora.Dictionary(processed_corpus)\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "    \n",
    "    print(\"train models\")\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    \n",
    "    test_string = \"購物 樂桃 香草 潛水 OIST 九州 象鼻岩 自駕\"\n",
    "    a = tfidf[dictionary.doc2bow(test_string.lower().split())]\n",
    "    print(test_string,\"\",a)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"initialize\")\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
