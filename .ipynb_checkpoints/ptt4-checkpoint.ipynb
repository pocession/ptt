{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize\n",
      "run main\n",
      "Enter which sub forum you want to scrape data from: Japan_Travel\n",
      "How many pages do you want: 2\n",
      "get data from Japan_Travel\n",
      "get index_page\n",
      "get pagelist\n",
      "Total page number is 6551.\n",
      "Get all url of pages ... done\n",
      "['https://www.ptt.cc/bbs/Japan_Travel/index.html', 'https://www.ptt.cc/bbs/Japan_Travel/index6550.html']\n",
      "get title\n",
      "Now get title from https://www.ptt.cc/bbs/Japan_Travel/index.html\n",
      "get reveiw count\n",
      "Now get reveiw count from https://www.ptt.cc/bbs/Japan_Travel/index.html\n",
      "get article url\n",
      "Now get reveiw url from https://www.ptt.cc/bbs/Japan_Travel/index.html\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6874ce84abf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initialize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-6874ce84abf6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0marticle_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mreview_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0marticle_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6874ce84abf6>\u001b[0m in \u001b[0;36marticle_url\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0marticle_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0marticle_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marticle_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mid_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Some practice for scraping ptt\n",
    "# This python scapper allows you to get article title, review count and url of this article from ptt\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Enter the subforum which you want to scrape data from\n",
    "# Subforum name is case sensitive\n",
    "def subforum():\n",
    "    name = input(\"Enter which sub forum you want to scrape data from: \")\n",
    "    page = int(input(\"How many pages do you want: \"))\n",
    "    print(\"get data from {a}\".format(a = name))\n",
    "    return(name, page)\n",
    "\n",
    "\n",
    "# Send http request to server\n",
    "def Response(a):\n",
    "    url = a\n",
    "    response = requests.get(url, cookies={'over18': '1'}) \n",
    "    result = response.text\n",
    "    return(result)\n",
    "\n",
    "\n",
    "# Return contend of the first page, index_page, which is a bs4.BeautifulSoup object\n",
    "def index_page(a,b):\n",
    "    print(\"get index_page\")\n",
    "    name = a\n",
    "    page = b\n",
    "    url = \"https://www.ptt.cc/bbs/\"+name+\"/index.html\"\n",
    "    data = Response(url)\n",
    "    index = BeautifulSoup(data, 'html.parser')\n",
    "    return (index)\n",
    "\n",
    "\n",
    "# Create a list to store url of all scrapped pages\n",
    "# This list is a index for scrapping contends\n",
    "def pagelist(index,a,b):\n",
    "    print(\"get pagelist\")\n",
    "    name = a\n",
    "    page = b\n",
    "    page_list = []\n",
    "    # Store the first page\n",
    "    a = index.find_all('a', attrs={'class':'btn wide'})\n",
    "    page_number = a[1]['href']\n",
    "    page_scrapped = \"https://www.ptt.cc/bbs/\"+name+\"/index.html\"\n",
    "    page_list.append(page_scrapped)\n",
    "    \n",
    "    # Get page number\n",
    "    # Extract page number by replace non-integer characters with NONE\n",
    "    # Plus 1 page to page number for index\n",
    "    fix = \"/bbs/{url}/index\".format(url = name)\n",
    "    page_number = int(page_number.replace(\"{a}\".format(a = fix),\"\").replace(\".html\",\"\"))\n",
    "    print(\"Total page number is {a}.\".format(a = page_number + 1))\n",
    "    \n",
    "    # Get url of pages, index_page is included\n",
    "    k = 0\n",
    "    \n",
    "    # -1 is to prevent to get extra pages\n",
    "    while k < page - 1:\n",
    "        page_scrapped_x = page_number - (k)\n",
    "        page_scrapped_y = str(page_scrapped_x)\n",
    "        page_scrapped_y = \"https://www.ptt.cc/bbs/\"+name+\"/index\"+page_scrapped_y+\".html\"\n",
    "        page_list.append(page_scrapped_y)\n",
    "        k += 1\n",
    "    print(\"Get all url of pages ... done\")\n",
    "    return(page_list)\n",
    "    \n",
    "\n",
    "# Get article title\n",
    "def title(a):\n",
    "    print(\"get title\")\n",
    "    url = a\n",
    "    print(\"Now get title from {a}\".format(a = url))\n",
    "    data = Response(url)\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    article_title = soup.find_all('div', attrs={'class':'title'})\n",
    "    title_list=[]\n",
    "    i = 0\n",
    "    while i < len(article_title):\n",
    "        element = article_title[i].text\n",
    "        element = element.replace(\"\\n\",\"\").replace(\" \",\"\").replace(\"\\t\",\"\").replace(\",\",\"\")\n",
    "        title_list.append(element)\n",
    "        i += 1\n",
    "    return(title_list)\n",
    "\n",
    "\n",
    "# Get article review count\n",
    "def review(a):\n",
    "    print(\"get reveiw count\")\n",
    "    url = a\n",
    "    print(\"Now get reveiw count from {a}\".format(a = url))\n",
    "    data = Response(url)\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    article_review = soup.find_all('div', attrs={'class':'nrec'})\n",
    "    review_list=[]\n",
    "    i = 0\n",
    "    while i < len(article_review):\n",
    "        element = article_review[i].text\n",
    "        if element == '':\n",
    "            element = element.replace('','0')\n",
    "        if element == 'çˆ†':\n",
    "            element = '99'\n",
    "        review_list.append(element)\n",
    "        i += 1\n",
    "    return(review_list)\n",
    "        \n",
    "\n",
    "# Get article url\n",
    "# This url also serves as article id in our database\n",
    "def article_url(a):\n",
    "    print(\"get article url\")\n",
    "    url = a\n",
    "    print(\"Now get reveiw url from {a}\".format(a = url))\n",
    "    data = Response(url)\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    article_link = soup.find_all('div', attrs={'class':'title'})\n",
    "    link_list=[]\n",
    "    id_list=[]\n",
    "    i = 0\n",
    "    \n",
    "    # Create the first list to save link, and make it as string\n",
    "    while i < len(article_link):\n",
    "        element = article_link[i]\n",
    "        link_list.append(str(element))\n",
    "        i += 1\n",
    "    \n",
    "    # Create the second list to extract link\n",
    "    # re module could not process data in a workflow\n",
    "    \n",
    "    j = 0\n",
    "    while j < len(link_list):\n",
    "        element = link_list[j]\n",
    "        article_id = re.findall(\"[a-zA-Z0-9]{1}.[a-zA-Z0-9]{10}.[a-zA-Z0-9]{1}.[a-zA-Z0-9]{3}.html\",element)\n",
    "        id_list.append(article_id)\n",
    "        j += 1\n",
    "    print(id_list)    \n",
    "    return(id_list)\n",
    "\n",
    "       \n",
    "# Write it into a csv file\n",
    "def write(a,b):\n",
    "    print(\"Now writing data into csv file\")\n",
    "    name = a\n",
    "    write_list = b\n",
    "    csvfile = open(\"{a}.csv\".format(a = name),\"w\",newline='')\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"title\", \"review_count\", \"id\"])\n",
    "    i = 0\n",
    "    while i < len(write_list):\n",
    "        csvrow = write_list[i]\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(csvrow)\n",
    "        i += 1\n",
    "    csvfile.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"run main\")\n",
    "    name,page = subforum()\n",
    "    soup=index_page(name,page)\n",
    "    page_list=pagelist(soup,name,page)\n",
    "    print(page_list)\n",
    "    i = 0\n",
    "    data = []\n",
    "    while i < page:\n",
    "        article_title = title(page_list[i])\n",
    "        review_count = review(page_list[i])\n",
    "        article_id = article_url(page_list[i])\n",
    "        j = 0\n",
    "        while j < len(article_title):\n",
    "            row = [article_title[j], review_count[j], article_id[j]]\n",
    "            data.append(row)\n",
    "            j += 1\n",
    "        i += 1\n",
    "    write(name,data)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"initialize\")\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
