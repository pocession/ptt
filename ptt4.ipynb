{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize\n",
      "run main\n",
      "Enter which sub forum you want to scrape data from: Japan_Travel\n",
      "How many pages do you want: 2\n",
      "get data from Japan_Travel\n",
      "get index_page\n",
      "get pagelist\n",
      "Total page number is 6520.\n",
      "Get all url of pages ... done\n",
      "['https://www.ptt.cc/bbs/Japan_Travel/index.html', 'https://www.ptt.cc/bbs/Japan_Travel/index6519.html']\n",
      "get title\n",
      "Now get data from https://www.ptt.cc/bbs/Japan_Travel/index.html\n",
      "get title\n",
      "Now get data from https://www.ptt.cc/bbs/Japan_Travel/index6519.html\n",
      "[['[徵人]3/5-3/9沖繩那霸吃燒肉', '[資訊]Skyliner+計程車直達東京市區飯店超划算', '[讓售]東京地鐵72小時搭車券', '[食記]北海道小樽-PopuraFarm哈密瓜冰', '<bossisgod>(未附圖n)', '[遊記]一日橫越黑部立山阿爾卑斯山脈路線', '[遊記]分享跨年樂桃機票、OTS租車費用(RA等級)', '[心得]2019年版京都賞櫻初心者指南:基礎篇', '[食記]金澤21世紀美術館周邊美食：更科藤井', '[問題]東京03/08-03/17行程請益', '[問題]SONYXperiaXZ3適用30天sim卡', '[徵人]4/15-16大阪or京都', '[遊記]山形藏王溫泉滑雪場悠遊滑行樹冰間', '[問題]京阪交通行程請益', '[遊記]東京迪士尼樂園|海洋攻略分享', '[徵人]4/7東京巨蛋看球賽為王柏融應援', '[公告]日本旅遊板板規107.06', '[公告]108.2實況回報(含天氣/物候/穿著詢問)', '[資訊]樂桃七周年促銷(日本發777日幣)', '[資訊]捷星台灣發促銷(已開始)', '[資訊]香草促銷今晚11點起KIX/OKA220~'], ['[問題]5月沖繩四天三夜行程請益', '[讓售]東京地鐵72小時券', '[問題]中部北陸6天5夜行程安排', '[問題]東京五月六天五夜行程', '[問題]名古屋高山白川鄉賞櫻行程請益', '[問題]請推薦防滑鞋(北海道)', '[遊記]西日本櫻花飛舞:奈良公園.元興寺.興福寺', '[遊記]川越一日遊懶人包/美食交通冰川神社整理', '[問題]淺草住宿請益', '[問題]雞繁接受一個人用餐嗎？', '[讓售](售出)關西機場→難波站南海電鐵*2', '[遊記]澀谷川大廈ShibuyaStream', '[問題]函館、小樽、札幌8天7夜賞櫻行程請益', '[問題]金澤跟高山的取捨?', '[心得]大阪轉蛋日本傳統文化十代目松本幸四郎', '[食記]大阪米其林二星カハラ超越期待的美味', '[食記]伊達牛舌本舖東京車站GRANROOF店', '[問題]Anyfonewifi機租借押金', '[遊記]日光輪王寺大猷院．德川家光的陵墓', 'Re:[遊記]北九州與關西孝親行心得與反省']]\n"
     ]
    }
   ],
   "source": [
    "# Some practice for scraping ptt\n",
    "# This python scapper allows you to get article title, review count and url of this article from ptt\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Enter the subforum which you want to scrape data from\n",
    "# Subforum name is case sensitive\n",
    "def subforum():\n",
    "    name = input(\"Enter which sub forum you want to scrape data from: \")\n",
    "    page = int(input(\"How many pages do you want: \"))\n",
    "    print(\"get data from {a}\".format(a = name))\n",
    "    return(name, page)\n",
    "\n",
    "\n",
    "# Send http request to server\n",
    "def Response(a):\n",
    "    url = a\n",
    "    response = requests.get(url, cookies={'over18': '1'}) \n",
    "    result = response.text\n",
    "    return(result)\n",
    "\n",
    "\n",
    "# Return contend of the first page, index_page, which is a bs4.BeautifulSoup object\n",
    "def index_page(a,b):\n",
    "    print(\"get index_page\")\n",
    "    name = a\n",
    "    page = b\n",
    "    url = \"https://www.ptt.cc/bbs/\"+name+\"/index.html\"\n",
    "    data = Response(url)\n",
    "    index = BeautifulSoup(data, 'html.parser')\n",
    "    return (index)\n",
    "\n",
    "\n",
    "# Create a list to store url of all scrapped pages\n",
    "# This list is a index for scrapping contends\n",
    "def pagelist(index,a,b):\n",
    "    print(\"get pagelist\")\n",
    "    name = a\n",
    "    page = b\n",
    "    page_list = []\n",
    "    # Store the first page\n",
    "    a = index.find_all('a', attrs={'class':'btn wide'})\n",
    "    page_number = a[1]['href']\n",
    "    page_scrapped = \"https://www.ptt.cc/bbs/\"+name+\"/index.html\"\n",
    "    page_list.append(page_scrapped)\n",
    "    \n",
    "    # Get page number\n",
    "    # Extract page number by replace non-integer characters with NONE\n",
    "    # Plus 1 page to page number for index\n",
    "    fix = \"/bbs/{url}/index\".format(url = name)\n",
    "    page_number = int(page_number.replace(\"{a}\".format(a = fix),\"\").replace(\".html\",\"\"))\n",
    "    print(\"Total page number is {a}.\".format(a = page_number + 1))\n",
    "    \n",
    "    # Get url of pages, index_page is included\n",
    "    k = 0\n",
    "    \n",
    "    # -1 is to prevent to get extra pages\n",
    "    while k < page - 1:\n",
    "        page_scrapped_x = page_number - (k)\n",
    "        page_scrapped_y = str(page_scrapped_x)\n",
    "        page_scrapped_y = \"https://www.ptt.cc/bbs/\"+name+\"/index\"+page_scrapped_y+\".html\"\n",
    "        page_list.append(page_scrapped_y)\n",
    "        k += 1\n",
    "    print(\"Get all url of pages ... done\")\n",
    "    return(page_list)\n",
    "    \n",
    "\n",
    "# Get each article information, including title, review counts and url\n",
    "def title(a):\n",
    "    print(\"get title\")\n",
    "    url = a\n",
    "    print(\"Now get data from {a}\".format(a = url))\n",
    "    data = Response(url)\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    article_title = soup.find_all('div', attrs={'class':'title'})\n",
    "    title_list=[]\n",
    "    i = 0\n",
    "    while i < len(article_title):\n",
    "        title_element = article_title[i].text\n",
    "        title_element = title_element.replace(\"\\n\",\"\").replace(\" \",\"\").replace(\"\\t\",\"\").replace(\",\",\"\")\n",
    "        title_list.append(title_element)\n",
    "        i += 1\n",
    "    return(title_list)\n",
    "\n",
    "\n",
    "# Write it into a csv file\n",
    "def write(a,b):\n",
    "    write_list = a\n",
    "    name = b\n",
    "    csvfile = open(\"{a}.csv\".format(a = name),\"w\",newline='')\n",
    "    i = 0\n",
    "    while i < len(write_list):\n",
    "        csvrow = write_list[i]\n",
    "        writer = csv.writer(csvfile, delimiter='\\n')\n",
    "        writer.writerow(csvrow)\n",
    "        i += 1\n",
    "    csvfile.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"run main\")\n",
    "    name,page = subforum()\n",
    "    soup=index_page(name,page)\n",
    "    page_list=pagelist(soup,name,page)\n",
    "    print(page_list)\n",
    "    i = 0\n",
    "    title_list = []\n",
    "    while i < page:\n",
    "        article_title = title(page_list[i])\n",
    "        title_list.append(article_title)\n",
    "        i += 1\n",
    "    write(title_list,name)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"initialize\")\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
